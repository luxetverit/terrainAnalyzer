import datetime
import gc
import io
import platform
import tempfile
import zipfile
from pathlib import Path

import geopandas as gpd
import matplotlib.patches as mpatches
import matplotlib.patheffects as path_effects
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import shapefile  # Use the newly installed pyshp library
import streamlit as st
from matplotlib.colors import BoundaryNorm, ListedColormap
from matplotlib.patches import Polygon, Rectangle
from scipy.ndimage import zoom
from shapely import wkb
from shapely.geometry.base import BaseGeometry
from shapely.ops import transform

from utils.color_palettes import get_landcover_colormap, get_palette
from utils.plot_helpers import (add_north_arrow, add_scalebar_vector,
                                adjust_ax_limits,
                                calculate_accurate_scalebar_params,
                                create_hillshade, create_padded_fig_ax,
                                draw_accurate_scalebar, generate_aspect_bins,
                                generate_custom_intervals,
                                generate_slope_intervals)
from utils.theme_util import apply_styles


# --- Helper function for SHP export using pyshp ---
def create_shapefile_zip(gdf: gpd.GeoDataFrame, base_filename: str) -> io.BytesIO | None:
    """Converts a GeoDataFrame to a zipped Shapefile in memory using the pyshp library."""
    if gdf.empty:
        return None

    gdf = gdf.copy()

    # --- [FINAL CLIP] Ensure output matches original user boundary ---
    if 'gdf' in st.session_state:
        original_gdf = st.session_state.gdf
        if not original_gdf.empty:
            # Ensure CRS match before clipping
            if gdf.crs != original_gdf.crs:
                gdf = gdf.to_crs(original_gdf.crs)
            
            # Perform the clip
            gdf = gpd.clip(gdf, original_gdf, keep_geom_type=True)
            if gdf.empty:
                st.warning("ìµœì¢… í´ë¦¬í•‘ í›„ SHP íŒŒì¼ë¡œ ë³€í™˜í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
    # --- End of Final Clip ---

    # Step 1: Force all elements to be geometry objects, correctly handling WKB.
    def force_to_geometry(geom):
        if isinstance(geom, str):
            try:
                return wkb.loads(geom, hex=True)
            except Exception:
                return None
        return geom if isinstance(geom, BaseGeometry) else None
    gdf['geometry'] = gdf['geometry'].apply(force_to_geometry)

    # Step 2: Force all geometries to 2D.
    def drop_z(geom):
        if geom is None or not geom.has_z:
            return geom
        return transform(lambda x, y, z=None: (x, y), geom)
    gdf['geometry'] = gdf['geometry'].apply(drop_z)

    # Step 3: Filter out any null or invalid geometries.
    gdf = gdf[gdf.geometry.notna() & ~gdf.geometry.is_empty &
              gdf.geometry.is_valid]
    if gdf.empty:
        st.warning("SHP íŒŒì¼ë¡œ ë³€í™˜í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° ì •ì œ í›„)")
        return None

    # Step 4: Truncate column names for Shapefile compatibility.
    gdf.columns = [str(col) for col in gdf.columns]
    rename_dict = {}
    for col in gdf.columns:
        if len(col.encode('utf-8')) > 10:
            new_col = col.encode('utf-8')[:10].decode('utf-8', 'ignore')
            i = 1
            final_col = new_col
            while final_col in gdf.columns or final_col in rename_dict.values():
                suffix = f"_{i}"
                final_col = f"{col.encode('utf-8')[:10-len(suffix.encode('utf-8'))].decode('utf-8', 'ignore')}{suffix}"
                i += 1
            rename_dict[col] = final_col
    if rename_dict:
        gdf = gdf.rename(columns=rename_dict)

    # Step 5: Write to shapefile using pyshp.
    with tempfile.TemporaryDirectory() as tmpdir:
        shp_path = str(Path(tmpdir) / f"{base_filename}.shp")
        try:
            with shapefile.Writer(shp_path) as w:
                w.autoBalance = 1  # Ensure consistency

                # Define fields from GeoDataFrame columns
                for col_name, dtype in gdf.dtypes.items():
                    if col_name.lower() == 'geometry':
                        continue
                    if pd.api.types.is_integer_dtype(dtype):
                        w.field(col_name, 'N')
                    elif pd.api.types.is_float_dtype(dtype):
                        w.field(col_name, 'F')
                    elif pd.api.types.is_datetime64_any_dtype(dtype):
                        w.field(col_name, 'D')
                    else:
                        w.field(col_name, 'C', size=254)

                # Write geometries and records
                for index, row in gdf.iterrows():
                    w.shape(row.geometry)

                    record_values = []
                    for col_name in gdf.columns:
                        if col_name.lower() == 'geometry':
                            continue

                        value = row[col_name]

                        # Handle potential NaN values before writing the record
                        if pd.isna(value):
                            dtype = gdf[col_name].dtype
                            if pd.api.types.is_integer_dtype(dtype):
                                value = 0
                            elif pd.api.types.is_float_dtype(dtype):
                                value = 0.0
                            else:
                                value = ''  # Default for strings/other types

                        record_values.append(value)

                    w.record(*record_values)

            # Zip the created files
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                for file_path in Path(tmpdir).glob(f'{base_filename}.*'):
                    zip_file.write(file_path, arcname=file_path.name)
            zip_buffer.seek(0)
            return zip_buffer

        except Exception as e:
            st.error(f"pyshp ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ SHP íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None


# --- Prepare base filename for downloads ---
uploaded_file_name = st.session_state.get('uploaded_file_name', 'untitled')
base_filename = Path(uploaded_file_name).stem
timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')

plot_figures = {}
shp_buffers = {}

# --- 0. Matplotlib Font Configuration ---
if platform.system() == 'Windows':
    plt.rc('font', family='Malgun Gothic')
else:
    # For Linux, ensure 'NanumGothic' is installed.
    # sudo apt-get install -y fonts-nanum*
    plt.rc('font', family='NanumGothic')
plt.rcParams['axes.unicode_minus'] = False

# --- 1. Page Configuration and Styling ---
st.set_page_config(page_title="ë¶„ì„ ê²°ê³¼ - ì§€í˜• ë¶„ì„ ì„œë¹„ìŠ¤",
                   page_icon="ğŸ“Š",
                   layout="wide",
                   initial_sidebar_state="collapsed")
apply_styles()

# --- 2. Session State Check ---
if 'dem_results' not in st.session_state:
    st.warning("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í™ˆ í˜ì´ì§€ë¡œ ëŒì•„ê°€ ë¶„ì„ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    if st.button("í™ˆìœ¼ë¡œ ëŒì•„ê°€ê¸°"):
        st.switch_page("app.py")
    st.stop()

# --- 3. Data Loading from Session ---
dem_results = st.session_state.dem_results
selected_types = st.session_state.get('selected_analysis_types', [])
matched_sheets = st.session_state.get('matched_sheets', [])
analysis_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")

# --- 4. Page Header ---
cols = st.columns([0.95, 0.05])
with cols[0]:
    st.markdown('''<div class="page-header" style="margin-top: -1.5rem;"><h1>ë¶„ì„ ê²°ê³¼</h1><p>ì„ íƒí•˜ì‹  í•­ëª©ì— ëŒ€í•œ ì§€í˜• ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.</p></div>''',
                unsafe_allow_html=True)
with cols[1]:
    if st.button("ğŸ ", help="í™ˆ í™”ë©´ìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.", use_container_width=True):
        # Clean up temporary TIF files before clearing session state
        if 'dem_results' in st.session_state:
            for analysis_type in st.session_state.dem_results:
                results = st.session_state.dem_results.get(analysis_type, {})
                tif_path = results.get('tif_path')
                if tif_path and Path(tif_path).exists():
                    try:
                        Path(tif_path).unlink()
                    except OSError as e:
                        st.warning(f".tif íŒŒì¼ì„ ì‚­ì œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

        for key in list(st.session_state.keys()):
            if key != 'upload_counter':
                del st.session_state[key]
        st.switch_page("app.py")

# --- 6. 2D Analysis Results (in Tabs) ---
st.markdown("### ğŸ“ˆ 2D ìƒì„¸ ë¶„ì„ ê²°ê³¼")
analysis_map = {
    'elevation': {'title': "í‘œê³  ë¶„ì„", 'unit': "m", 'binned_label': "í‘œê³  êµ¬ê°„ë³„ ë©´ì "},
    'slope': {'title': "ê²½ì‚¬ ë¶„ì„", 'unit': "Â°", 'binned_label': "ê²½ì‚¬ êµ¬ê°„ë³„ ë©´ì "},
    'aspect': {'title': "ê²½ì‚¬í–¥ ë¶„ì„", 'unit': "Â°", 'binned_label': "ê²½ì‚¬í–¥ êµ¬ê°„ë³„ ë©´ì "},
    'soil': {'title': "í† ì–‘ë„ ë¶„ì„", 'unit': "mÂ²", 'binned_label': "í† ì–‘ ì¢…ë¥˜ë³„ ë©´ì ", 'class_col': 'soilsy', 'legend_title': 'í† ì–‘ë„(Soilsy)'},
    'hsg': {'title': "ìˆ˜ë¬¸í•™ì  í† ì–‘êµ°", 'unit': "mÂ²", 'binned_label': "HSG ë“±ê¸‰ë³„ ë©´ì ", 'class_col': 'hg', 'legend_title': 'í† ì–‘êµ°(HSG)'},
    'landcover': {'title': "í† ì§€í”¼ë³µë„", 'unit': "mÂ²", 'binned_label': "í† ì§€í”¼ë³µë³„ ë©´ì ", 'class_col': 'l2_name', 'legend_title': 'í† ì§€í”¼ë³µë„(Landcover)'},
}
valid_selected_types = [t for t in selected_types if t in dem_results]

if not valid_selected_types:
    st.info("í‘œì‹œí•  2D ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    # Loop through each analysis type and display its results sequentially
    for analysis_type in valid_selected_types:
        with st.container(border=True):
            st.markdown(
                f"### ğŸ“ˆ {analysis_map.get(analysis_type, {}).get('title', analysis_type)}")

            results = dem_results[analysis_type]
            stats = results.get('stats')
            grid = results.get('grid')
            gdf = results.get('gdf')

            # [OPTIMIZATION 1] Halve memory usage of the grid by changing data type
            if grid is not None:
                grid = grid.astype(np.float32)

                # [OPTIMIZATION 2 & Scalebar Fix] Dynamic Downsampling and Pixel Size Adjustment
                effective_pixel_size = st.session_state.get('pixel_size', 1.0)
                PIXEL_THRESHOLD = 5_000_000  # Approx 2500x2000 image
                if grid.size > PIXEL_THRESHOLD:
                    downsample_factor = (PIXEL_THRESHOLD / grid.size) ** 0.5
                    st.info(
                        f"â„¹ï¸ ë¶„ì„ ì˜ì—­ì´ ë§¤ìš° ì»¤ì„œ ì‹œê°í™” í•´ìƒë„ë¥¼ ì›ë³¸ì˜ {downsample_factor:.1%}ë¡œ ìë™ ì¡°ì •í•©ë‹ˆë‹¤.")
                    # order=1 for bilinear interpolation
                    grid = zoom(grid, downsample_factor, order=1)
                    # Adjust pixel size to match the new resolution for accurate scale bar
                    effective_pixel_size = effective_pixel_size / downsample_factor

            if stats:
                st.markdown("#### ìš”ì•½ í†µê³„")
                cols = st.columns(3)
                cols[0].metric(
                    label="ìµœì†Œê°’", value=f"{stats.get('min', 0):.2f} {analysis_map.get(analysis_type, {}).get('unit', '')}")
                cols[1].metric(
                    label="ìµœëŒ€ê°’", value=f"{stats.get('max', 0):.2f} {analysis_map.get(analysis_type, {}).get('unit', '')}")
                cols[2].metric(
                    label="í‰ê· ê°’", value=f"{stats.get('mean', 0):.2f} {analysis_map.get(analysis_type, {}).get('unit', '')}")

                title = analysis_map.get(analysis_type, {}).get(
                    'title', analysis_type)
                with st.spinner(f"'{title}' ë¶„ì„ë„ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    # --- Unified DEM Analysis Plotting ---

                    # Retrieve all visualization info from the results dictionary
                    bins = results.get('bins')
                    labels = results.get('labels')
                    palette_name = results.get('palette_name')

                    if not all([bins, labels, palette_name]):
                        st.warning(f"'{title}'ì— ëŒ€í•œ ì‹œê°í™” ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue

                    palette_data = get_palette(palette_name)
                    if not palette_data:
                        st.warning(f"'{palette_name}' íŒ”ë ˆíŠ¸ë¥¼ DBì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue

                    # For aspect, reorder the palette to put 'Flat' first, matching the data processing.
                    if analysis_type == 'aspect':
                        flat_label_item = None
                        for item in palette_data:
                            if item['bin_label'].strip().lower() == 'flat':
                                flat_label_item = item
                                break

                        if flat_label_item:
                            palette_data.remove(flat_label_item)
                            palette_data.insert(0, flat_label_item)

                    colors = [item['hex_color'] for item in palette_data]

                    # Create colormap and normalization
                    cmap = ListedColormap(colors)
                    norm = BoundaryNorm(bins, cmap.N)

                    # --- Plotting ---
                    fig, ax = create_padded_fig_ax(figsize=(10, 8))
                    ax.set_title(title, fontsize=16, pad=20)

                    use_hillshade = False
                    if analysis_type == 'elevation':
                        use_hillshade = st.toggle(
                            "ìŒì˜ê¸°ë³µë„ ì¤‘ì²©", value=True, key=f"hillshade_{analysis_type}")
                        if use_hillshade:
                            hillshade = create_hillshade(grid)
                            rgba_data = cmap(norm(grid))
                            intensity = 0.7
                            hillshade_adjusted = 0.5 + hillshade * intensity
                            for i in range(3):
                                rgba_data[:, :, i] *= hillshade_adjusted
                            valid_mask = ~np.isnan(grid)
                            rgba_data[~valid_mask, 3] = 0
                            ax.imshow(np.clip(rgba_data, 0, 1), origin='upper')
                            del hillshade, hillshade_adjusted, rgba_data
                            gc.collect()
                        else:
                            ax.imshow(np.ma.masked_invalid(
                                grid), cmap=cmap, norm=norm)
                    else:
                        ax.imshow(np.ma.masked_invalid(
                            grid), cmap=cmap, norm=norm)

                    # ë“±ê³ ì„  ì¶”ê°€ (50m ê°„ê²©)
                    if analysis_type == 'elevation':
                        min_val, max_val = stats.get(
                            'min', 0), stats.get('max', 1)
                        level_interval = 100
                        start_level = np.ceil(
                            min_val / level_interval) * level_interval
                        end_level = np.floor(
                            max_val / level_interval) * level_interval

                        if start_level < end_level:
                            contour_levels = np.arange(
                                start_level, end_level + 1, level_interval)
                        else:
                            contour_levels = np.linspace(min_val, max_val, 10)

                        contour = ax.contour(
                            grid, levels=contour_levels, colors='k', alpha=0.3, linewidths=0.7)

                        # ë“±ê³ ì„  ë¼ë²¨ ì¶”ê°€ (50m ê°„ê²©)
                        label_interval = 100
                        start_label_level = np.ceil(
                            min_val / label_interval) * label_interval
                        end_label_level = np.floor(
                            max_val / label_interval) * label_interval

                        if start_label_level < end_label_level:
                            label_levels = np.arange(
                                start_label_level, end_label_level + 1, label_interval)
                        else:
                            # 100m ê°„ê²© ë¼ë²¨ì„ í‘œì‹œí•  ìˆ˜ ì—†ìœ¼ë©´ ëª¨ë“  ë“±ê³ ì„ ì— ë¼ë²¨ í‘œì‹œ
                            label_levels = contour_levels

                        clabels = ax.clabel(
                            contour, levels=label_levels, inline=True, fontsize=8, fmt='%.0f')
                        # [Readability Improvement] Add a white stroke to contour labels
                        plt.setp(clabels, fontweight='bold', path_effects=[
                                 path_effects.withStroke(linewidth=3, foreground='w')])

                    # --- Legend and Map Elements ---
                    patches = [mpatches.Patch(color=color, label=label)
                               for color, label in zip(colors, labels)]
                    legend_titles = {
                        'elevation': 'í‘œê³ (Elevation)',
                        'slope': 'ê²½ì‚¬(Slope)',
                        'aspect': 'ê²½ì‚¬í–¥(Aspect)'
                    }
                    legend_title = legend_titles.get(analysis_type, 'ë²”  ë¡€')
                    legend = ax.legend(handles=patches, title=legend_title,
                                       bbox_to_anchor=(0.1, 0.1), loc='lower left',
                                       bbox_transform=fig.transFigure,
                                       fontsize='small', frameon=True, framealpha=1,
                                       edgecolor='black')
                    legend.get_title().set_fontweight('bold')

                    # Force edge color and width on legend patches
                    for legend_patch in legend.get_patches():
                        legend_patch.set_edgecolor('black')
                        legend_patch.set_linewidth(0.7)

                    adjust_ax_limits(ax)
                    add_north_arrow(ax)
                    scale_params = calculate_accurate_scalebar_params(
                        effective_pixel_size, grid.shape, 50, fig, ax)
                    draw_accurate_scalebar(
                        fig, ax, effective_pixel_size, scale_params, grid.shape)
                    ax.axis('off')

                    # --- Display and Download ---
                    img_buffer = io.BytesIO()
                    fig.savefig(img_buffer, format='png',
                                bbox_inches='tight', dpi=150)
                    img_buffer.seek(0)
                    st.pyplot(fig)
                    plot_figures[analysis_type] = fig

            elif gdf is not None and not gdf.empty:
                title = analysis_map.get(
                    analysis_type, {}).get('title', analysis_type)
                with st.spinner(f"'{title}' ë¶„ì„ë„ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    fig, ax = create_padded_fig_ax(figsize=(10, 10))
                    type_info = analysis_map.get(analysis_type, {})
                    class_col = type_info.get('class_col')

                    # Landcover custom color logic
                    if analysis_type == 'landcover':
                        # Check if color map is loaded and required columns exist
                        color_map = get_landcover_colormap()
                        if color_map and 'l2_code' in gdf.columns and 'l2_name' in gdf.columns:
                            gdf['plot_color'] = gdf['l2_code'].map(color_map)
                            # Plot with the specified colors, handling potential missing colors
                            gdf.plot(ax=ax, color=gdf['plot_color'].fillna(
                                '#FFFFFF'), linewidth=0.5, edgecolor='k')

                            # Create a custom legend
                            unique_cats = gdf[['l2_code', 'l2_name']].drop_duplicates(
                            ).sort_values(by='l2_code')
                            patches = []
                            for _, row in unique_cats.iterrows():
                                code = row['l2_code']
                                name = row['l2_name']
                                # Default to white if not in map
                                color = color_map.get(code, '#FFFFFF')
                                patch = mpatches.Patch(
                                    color=color, label=f'{name}')
                                patches.append(patch)

                            if patches:
                                n_items = len(patches)
                                n_cols = (n_items + 19) // 20
                                legend = ax.legend(handles=patches, title=type_info.get('legend_title', 'ë¶„ë¥˜'),
                                                   bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small',
                                                   ncol=n_cols)
                                # Force edge color and width on legend patches
                                for legend_patch in legend.get_patches():
                                    legend_patch.set_edgecolor('black')
                                    legend_patch.set_linewidth(0.7)
                        else:
                            # Fallback for landcover if custom colors fail
                            gdf.plot(column=class_col, ax=ax, legend=True, categorical=True,
                                     legend_kwds={'title': type_info.get('legend_title', 'ë¶„ë¥˜'), 'bbox_to_anchor': (1.05, 1), 'loc': 'upper left'})

                    # Logic for other vector types (soil, hsg)
                    else:
                        if class_col and class_col in gdf.columns:
                            gdf_plot = gdf[gdf[class_col].notna()].copy()
                            unique_cats = sorted(gdf_plot[class_col].unique())

                            num_cats = len(unique_cats)
                            # Use 'tab20' for up to 20 categories, then a sampled colormap for more
                            if num_cats <= 20:
                                cmap = plt.cm.get_cmap('tab20', num_cats)
                                colors = cmap.colors
                            else:
                                cmap = plt.cm.get_cmap('viridis', num_cats)
                                colors = cmap(np.linspace(0, 1, num_cats))

                            color_map = {cat: color for cat,
                                         color in zip(unique_cats, colors)}
                            gdf_plot['plot_color'] = gdf_plot[class_col].map(
                                color_map)

                            gdf_plot.plot(
                                ax=ax, color=gdf_plot['plot_color'], linewidth=0.5, edgecolor='k')

                            patches = [mpatches.Patch(
                                color=color, label=cat) for cat, color in color_map.items()]

                            if patches:
                                n_items = len(patches)
                                n_cols = (n_items + 19) // 20
                                legend = ax.legend(handles=patches, title=type_info.get('legend_title', 'ë¶„ë¥˜'),
                                                   bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small',
                                                   ncol=n_cols)
                                # Force edge color and width on legend patches
                                for legend_patch in legend.get_patches():
                                    legend_patch.set_edgecolor('black')
                                    legend_patch.set_linewidth(0.7)
                        else:
                            gdf.plot(ax=ax)
                            if class_col:
                                st.warning(
                                    f"ì£¼ì˜: ë¶„ì„ì— í•„ìš”í•œ ë¶„ë¥˜ ì»¬ëŸ¼ '{class_col}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DB í…Œì´ë¸” ìŠ¤í‚¤ë§ˆë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(gdf.columns)}")
                            else:
                                st.warning("ë¶„ì„ ìœ í˜•ì— ëŒ€í•œ ë¶„ë¥˜ ì»¬ëŸ¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                    adjust_ax_limits(ax)
                    add_north_arrow(ax)

                    # --- Unify Scalebar to match DEM style ---
                    # 1. Get map dimensions in meters from the plot axes
                    x_min, x_max = ax.get_xlim()
                    map_width_m = x_max - x_min

                    # 2. Create a proxy for image shape and pixel size
                    #    (based on savefig dpi and figsize)
                    dpi = 150
                    # The figsize used in create_padded_fig_ax
                    figsize_w, figsize_h = (10, 10)
                    proxy_img_width_px = int(figsize_w * dpi)
                    proxy_img_height_px = int(figsize_h * dpi)
                    proxy_img_shape = (proxy_img_height_px, proxy_img_width_px)

                    # effective_pixel_size is meters/pixel
                    if proxy_img_width_px > 0:
                        effective_pixel_size = map_width_m / proxy_img_width_px
                    else:
                        effective_pixel_size = 1.0  # Fallback

                    # 3. Calculate and draw the accurate scalebar
                    scale_params = calculate_accurate_scalebar_params(
                        effective_pixel_size, proxy_img_shape, 50, fig, ax)
                    draw_accurate_scalebar(
                        fig, ax, effective_pixel_size, scale_params, proxy_img_shape)
                    ax.axis('off')

                    # --- Display and Store Buffer ---
                    img_buffer = io.BytesIO()
                    fig.savefig(img_buffer, format='png',
                                bbox_inches='tight', dpi=150)
                    img_buffer.seek(0)

                    st.pyplot(fig)
                    plot_figures[analysis_type] = fig

                    # --- SHP íŒŒì¼ ë²„í¼ ìƒì„± ë° ì €ì¥ ---
                    shp_zip_buffer = create_shapefile_zip(
                        gdf, f"{analysis_type}_{base_filename}"
                    )
                    if shp_zip_buffer:
                        shp_buffers[analysis_type] = {
                            "buffer": shp_zip_buffer,
                            "title": type_info.get('title', analysis_type)
                        }
                    # --- ë¡œì§ ì™„ë£Œ ---

            else:
                st.info("ì‹œê°í™”í•  2D ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.markdown("---")  # Add a separator between analyses
# --- 7. Summary and Final Download ---
st.markdown("### ğŸ“‹ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ")

# --- Generate Data for TXT Summary (for display) and CSV (for download) ---
summary_lines = []
csv_data = []

pixel_size = st.session_state.get('pixel_size', 1.0)
area_per_pixel = pixel_size * pixel_size

# --- Calculate Total Area for the report header ---
report_total_area_m2 = 0
area_source_type = None

# Prioritize 'elevation' for total area calculation
if 'elevation' in valid_selected_types:
    area_source_type = 'elevation'
elif valid_selected_types:
    area_source_type = valid_selected_types[0]

if area_source_type:
    results = dem_results[area_source_type]
    stats = results.get('stats')
    gdf = results.get('gdf')
    if stats:
        report_total_area_m2 = stats.get('area', 0) * area_per_pixel
    elif gdf is not None and not gdf.empty:
        if 'area' not in gdf.columns:
            gdf['area'] = gdf.geometry.area
        report_total_area_m2 = gdf.area.sum()

# General Info
summary_lines.append(f"ë¶„ì„ ì¼ì‹œ: {analysis_date}")
summary_lines.append(
    f"ë¶„ì„ ëŒ€ìƒ: {st.session_state.get('uploaded_file_name', 'N/A')}")
csv_data.append({'ë¶„ì„ êµ¬ë¶„': 'ê¸°ë³¸ ì •ë³´', 'í•­ëª©': 'ë¶„ì„ ì¼ì‹œ',
                'ê°’': analysis_date, 'ë‹¨ìœ„': '', 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})
csv_data.append({'ë¶„ì„ êµ¬ë¶„': 'ê¸°ë³¸ ì •ë³´', 'í•­ëª©': 'ë¶„ì„ ëŒ€ìƒ', 'ê°’': st.session_state.get(
    'uploaded_file_name', 'N/A'), 'ë‹¨ìœ„': '', 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})

if len(matched_sheets) > 20:
    summary_lines.append(f"ì‚¬ìš©ëœ ë„ì—½: {len(matched_sheets)}ê°œ")
    csv_data.append({'ë¶„ì„ êµ¬ë¶„': 'ê¸°ë³¸ ì •ë³´', 'í•­ëª©': 'ì‚¬ìš©ëœ ë„ì—½ ê°œìˆ˜', 'ê°’': len(
        matched_sheets), 'ë‹¨ìœ„': 'ê°œ', 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})
else:
    summary_lines.append(
        f"ì‚¬ìš©ëœ ë„ì—½: {len(matched_sheets)}ê°œ ({', '.join(matched_sheets)})")
    csv_data.append({'ë¶„ì„ êµ¬ë¶„': 'ê¸°ë³¸ ì •ë³´', 'í•­ëª©': 'ì‚¬ìš©ëœ ë„ì—½',
                    'ê°’': f"{len(matched_sheets)}ê°œ ({', '.join(matched_sheets)})", 'ë‹¨ìœ„': '', 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})

summary_lines.append(f"ì´ ë¶„ì„ ë©´ì : {int(report_total_area_m2):,} mÂ²")
csv_data.append({'ë¶„ì„ êµ¬ë¶„': 'ê¸°ë³¸ ì •ë³´', 'í•­ëª©': 'ì´ ë¶„ì„ ë©´ì ', 'ê°’': '', 'ë‹¨ìœ„': 'mÂ²',
                'ë©´ì (mÂ²)': f"{int(report_total_area_m2):,}", 'ë¹„ìœ¨(%)': ''})
summary_lines.append("")

# Analysis-specific Info
for analysis_type in valid_selected_types:
    stats = dem_results[analysis_type].get('stats')
    binned_stats = dem_results[analysis_type].get('binned_stats')
    gdf = dem_results[analysis_type].get('gdf')
    title_info = analysis_map.get(analysis_type, {})
    title = title_info.get('title', analysis_type)

    summary_lines.append(f"--- {title} ---")

    total_area_m2 = 0
    if stats:
        total_area_m2 = stats.get('area', 0) * area_per_pixel
        unit = title_info.get('unit', '')
        summary_lines.append(f"- ìµœì†Œê°’: {stats.get('min', 0):.2f} {unit}")
        summary_lines.append(f"- ìµœëŒ€ê°’: {stats.get('max', 0):.2f} {unit}")
        summary_lines.append(f"- í‰ê· ê°’: {stats.get('mean', 0):.2f} {unit}")

        csv_data.append({'ë¶„ì„ êµ¬ë¶„': title, 'í•­ëª©': 'ìµœì†Œê°’',
                        'ê°’': f"{stats.get('min', 0):.2f}", 'ë‹¨ìœ„': unit, 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})
        csv_data.append({'ë¶„ì„ êµ¬ë¶„': title, 'í•­ëª©': 'ìµœëŒ€ê°’',
                        'ê°’': f"{stats.get('max', 0):.2f}", 'ë‹¨ìœ„': unit, 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})
        csv_data.append({'ë¶„ì„ êµ¬ë¶„': title, 'í•­ëª©': 'í‰ê· ê°’',
                        'ê°’': f"{stats.get('mean', 0):.2f}", 'ë‹¨ìœ„': unit, 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})

    elif gdf is not None and not gdf.empty:
        if 'area' not in gdf.columns:
            gdf['area'] = gdf.geometry.area
        total_area_m2 = gdf.area.sum()

    if binned_stats:
        summary_lines.append(f"\n[{title_info.get('binned_label', 'êµ¬ê°„ë³„ í†µê³„')}]")
        for row in binned_stats:
            binned_area_m2 = row['area'] * area_per_pixel
            percentage = (binned_area_m2 / total_area_m2 *
                          100) if total_area_m2 > 0 else 0
            summary_lines.append(
                f"- {row['bin_range']}: {int(binned_area_m2):,} mÂ² ({percentage:.1f} %)")
            csv_data.append({'ë¶„ì„ êµ¬ë¶„': title, 'í•­ëª©': row['bin_range'], 'ê°’': '', 'ë‹¨ìœ„': '',
                            'ë©´ì (mÂ²)': f"{int(binned_area_m2):,}", 'ë¹„ìœ¨(%)': f"{percentage:.1f}"})

    if gdf is not None and not gdf.empty:
        class_col = title_info.get('class_col')
        if class_col and class_col in gdf.columns:
            # [CRITICAL FIX 2] Use dissolve to merge geometries first, then calculate area.
            # This correctly handles overlapping polygons from the source data.
            dissolved_gdf = gdf.dissolve(by=class_col)
            dissolved_gdf['area'] = dissolved_gdf.geometry.area
            dissolved_gdf = dissolved_gdf.sort_values(by='area', ascending=False)

            summary_lines.append(
                f"\n[{title_info.get('binned_label', 'ì¢…ë¥˜ë³„ í†µê³„')}]")
            for item, row in dissolved_gdf.iterrows():
                area = row['area']
                percentage = (area / total_area_m2 *
                              100) if total_area_m2 > 0 else 0
                summary_lines.append(
                    f"- {item}: {int(area):,} mÂ² ({percentage:.1f} %)")
                csv_data.append({'ë¶„ì„ êµ¬ë¶„': title, 'í•­ëª©': item, 'ê°’': '', 'ë‹¨ìœ„': '',
                                'ë©´ì (mÂ²)': f"{int(area):,}", 'ë¹„ìœ¨(%)': f"{percentage:.1f}"})
        else:
            # This part is tricky, as total area for GDF is already calculated above.
            # We just add the line to the text summary.
            summary_lines.append(f"- ì´ ë¶„ì„ ë©´ì : {int(total_area_m2):,} mÂ²")
            if class_col:
                summary_lines.append(
                    f"- (ìƒì„¸ ë©´ì  í†µê³„ë¥¼ ê³„ì‚°í•˜ë ¤ë©´ '{class_col}' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.)")

    summary_lines.append("")

# --- Create Display Text and CSV String ---
summary_text = "\n".join(summary_lines)
report_df = pd.DataFrame(csv_data)
csv_buffer = io.StringIO()
report_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
csv_string = csv_buffer.getvalue()


# Display Summary Text Area
st.text_area("", summary_text, height=400)

# --- Create Final ZIP and Download Button ---
zip_buffer = io.BytesIO()
with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
    # Add summary CSV to zip
    zip_file.writestr(
        f"analysis_summary_{timestamp}.csv", csv_string)

    # Add plot images to zip
    for analysis_type, fig in plot_figures.items():
        img_buffer = io.BytesIO()

        # Re-check toggle state for correct filename
        use_hillshade = st.session_state.get(
            f"hillshade_{analysis_type}", False)
        file_name_suffix = "_hillshade" if analysis_type == 'elevation' and use_hillshade else ""

        fig.savefig(img_buffer, format='png', bbox_inches='tight', dpi=150)
        img_buffer.seek(0)

        zip_file.writestr(
            f"{analysis_type}_analysis{file_name_suffix}.png", img_buffer.getvalue())
        plt.close(fig)  # Close the figure after saving

    # Add analysis TIF files to zip
    for analysis_type in valid_selected_types:
        results = dem_results.get(analysis_type, {})
        tif_path = results.get('tif_path')
        if tif_path and Path(tif_path).exists():
            zip_file.write(tif_path, arcname=f"{analysis_type}.tif")

zip_buffer.seek(0)

# --- 8. Final Download Section ---
st.markdown("### ğŸ“¥ ë‹¤ìš´ë¡œë“œ")

# --- Create a list of all download items ---
download_items = []

# Add the main ZIP download first
download_items.append({
    "label": "ğŸ“¥ ì‹œê°í™”ìë£Œ+ë¶„ì„ë³´ê³ ì„œ (ZIP)",
    "data": zip_buffer,
    "file_name": f"analysis_results_{base_filename}_{timestamp}.zip",
    "mime": "application/zip",
    "key": "main_zip_download",
    "help": "ë¶„ì„ ë¦¬í¬íŠ¸, ëª¨ë“  ë¶„ì„ë„(PNG), ëª¨ë“  ì›ë³¸ ë¶„ì„ íŒŒì¼(TIF)ì„ í•œë²ˆì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
})

# Add the individual SHP downloads
for analysis_type, data in shp_buffers.items():
    download_items.append({
        "label": f"ğŸ“¥ {data['title']} (SHP)",
        "data": data['buffer'],
        "file_name": f"{analysis_type}_{base_filename}_{timestamp}.zip",
        "mime": "application/zip",
        "key": f"shp_download_bottom_{analysis_type}",
        "help": f"{data['title']} ë¶„ì„ ê²°ê³¼ë¥¼ SHP íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
    })

# --- Create columns and display buttons ---
if download_items:
    cols = st.columns(len(download_items))
    for i, item in enumerate(download_items):
        with cols[i]:
            st.download_button(
                label=item["label"],
                data=item["data"],
                file_name=item["file_name"],
                mime=item["mime"],
                key=item["key"],
                use_container_width=True,
                help=item["help"]
            )

st.markdown("")  # Spacer

# --- Final Buttons ---
if st.button("ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘í•˜ê¸°", use_container_width=True):
    # Clean up temporary TIF files before clearing session state
    if 'dem_results' in st.session_state:
        for analysis_type in st.session_state.dem_results:
            results = st.session_state.dem_results.get(analysis_type, {})
            tif_path = results.get('tif_path')
            if tif_path and Path(tif_path).exists():
                try:
                    Path(tif_path).unlink()
                except OSError as e:
                    st.warning(f".tif íŒŒì¼ì„ ì‚­ì œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    for key in list(st.session_state.keys()):
        if key not in ['upload_counter']:
            del st.session_state[key]
    st.switch_page("app.py")
