import datetime
import gc
import io
import platform
import tempfile
import zipfile
from pathlib import Path

import geopandas as gpd
import matplotlib.patches as mpatches
import matplotlib.patheffects as path_effects
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import shapefile  # ìƒˆë¡œ ì„¤ì¹˜ëœ pyshp ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš©
import streamlit as st
from matplotlib.colors import BoundaryNorm, ListedColormap
from matplotlib.patches import Polygon, Rectangle
from scipy.ndimage import zoom
from shapely import wkb
from shapely.geometry.base import BaseGeometry
from shapely.ops import transform

from utils.color_palettes import get_landcover_colormap, get_palette
from utils.plot_helpers import (
    add_north_arrow,
    add_scalebar_vector,
    adjust_ax_limits,
    calculate_accurate_scalebar_params,
    create_hillshade,
    create_padded_fig_ax,
    draw_accurate_scalebar,
    generate_aspect_bins,
    generate_custom_intervals,
    generate_slope_intervals,
)
from utils.theme_util import apply_styles


# --- pyshpë¥¼ ì‚¬ìš©í•œ SHP ë‚´ë³´ë‚´ê¸° ë„ìš°ë¯¸ í•¨ìˆ˜ ---
def create_shapefile_zip(gdf: gpd.GeoDataFrame, base_filename: str) -> io.BytesIO | None:
    """GeoDataFrameì„ ë©”ëª¨ë¦¬ì—ì„œ pyshp ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ì••ì¶•ëœ Shapefileë¡œ ë³€í™˜í•©ë‹ˆë‹¤."""
    if gdf.empty:
        return None

    gdf = gdf.copy()

    # --- [ìµœì¢… í´ë¦½] ì¶œë ¥ì´ ì›ë³¸ ì‚¬ìš©ì ê²½ê³„ì™€ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸ ---
    if 'gdf' in st.session_state:
        original_gdf = st.session_state.gdf
        if not original_gdf.empty:
            # í´ë¦¬í•‘ ì „ CRS ì¼ì¹˜ í™•ì¸
            if gdf.crs != original_gdf.crs:
                gdf = gdf.to_crs(original_gdf.crs)
            
            # í´ë¦½ ìˆ˜í–‰
            gdf = gpd.clip(gdf, original_gdf, keep_geom_type=True)
            if gdf.empty:
                st.warning("ìµœì¢… í´ë¦¬í•‘ í›„ SHP íŒŒì¼ë¡œ ë³€í™˜í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
    # --- ìµœì¢… í´ë¦½ ì¢…ë£Œ ---

    # ë‹¨ê³„ 1: ëª¨ë“  ìš”ì†Œë¥¼ ì§€ì˜¤ë©”íŠ¸ë¦¬ ê°ì²´ë¡œ ê°•ì œ ë³€í™˜í•˜ê³  WKBë¥¼ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
    def force_to_geometry(geom):
        if isinstance(geom, str):
            try:
                return wkb.loads(geom, hex=True)
            except Exception:
                return None
        return geom if isinstance(geom, BaseGeometry) else None
    gdf['geometry'] = gdf['geometry'].apply(force_to_geometry)

    # ë‹¨ê³„ 2: ëª¨ë“  ì§€ì˜¤ë©”íŠ¸ë¦¬ë¥¼ 2Dë¡œ ê°•ì œ ë³€í™˜í•©ë‹ˆë‹¤.
    def drop_z(geom):
        if geom is None or not geom.has_z:
            return geom
        return transform(lambda x, y, z=None: (x, y), geom)
    gdf['geometry'] = gdf['geometry'].apply(drop_z)

    # ë‹¨ê³„ 3: nullì´ê±°ë‚˜ ìœ íš¨í•˜ì§€ ì•Šì€ ì§€ì˜¤ë©”íŠ¸ë¦¬ë¥¼ í•„í„°ë§í•©ë‹ˆë‹¤.
    gdf = gdf[gdf.geometry.notna() & ~gdf.geometry.is_empty &
              gdf.geometry.is_valid]
    if gdf.empty:
        st.warning("SHP íŒŒì¼ë¡œ ë³€í™˜í•  ìœ íš¨í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤. (ë°ì´í„° ì •ì œ í›„)")
        return None

    # ë‹¨ê³„ 4: Shapefile í˜¸í™˜ì„±ì„ ìœ„í•´ ì»¬ëŸ¼ ì´ë¦„ì„ ìë¦…ë‹ˆë‹¤.
    gdf.columns = [str(col) for col in gdf.columns]
    rename_dict = {}
    for col in gdf.columns:
        if len(col.encode('utf-8')) > 10:
            new_col = col.encode('utf-8')[:10].decode('utf-8', 'ignore')
            i = 1
            final_col = new_col
            while final_col in gdf.columns or final_col in rename_dict.values():
                suffix = f"_{i}"
                final_col = f"{col.encode('utf-8')[:10-len(suffix.encode('utf-8'))].decode('utf-8', 'ignore')}{suffix}"
                i += 1
            rename_dict[col] = final_col
    if rename_dict:
        gdf = gdf.rename(columns=rename_dict)

    # ë‹¨ê³„ 5: pyshpë¥¼ ì‚¬ìš©í•˜ì—¬ shapefileì— ì”ë‹ˆë‹¤.
    with tempfile.TemporaryDirectory() as tmpdir:
        shp_path = str(Path(tmpdir) / f"{base_filename}.shp")
        try:
            with shapefile.Writer(shp_path) as w:
                w.autoBalance = 1  # ì¼ê´€ì„± ë³´ì¥

                # GeoDataFrame ì»¬ëŸ¼ì—ì„œ í•„ë“œ ì •ì˜
                for col_name, dtype in gdf.dtypes.items():
                    if col_name.lower() == 'geometry':
                        continue
                    if pd.api.types.is_integer_dtype(dtype):
                        w.field(col_name, 'N')
                    elif pd.api.types.is_float_dtype(dtype):
                        w.field(col_name, 'F')
                    elif pd.api.types.is_datetime64_any_dtype(dtype):
                        w.field(col_name, 'D')
                    else:
                        w.field(col_name, 'C', size=254)

                # ì§€ì˜¤ë©”íŠ¸ë¦¬ ë° ë ˆì½”ë“œ ì‘ì„±
                for index, row in gdf.iterrows():
                    w.shape(row.geometry)

                    record_values = []
                    for col_name in gdf.columns:
                        if col_name.lower() == 'geometry':
                            continue

                        value = row[col_name]

                        # ë ˆì½”ë“œë¥¼ ì‘ì„±í•˜ê¸° ì „ì— ì ì¬ì ì¸ NaN ê°’ ì²˜ë¦¬
                        if pd.isna(value):
                            dtype = gdf[col_name].dtype
                            if pd.api.types.is_integer_dtype(dtype):
                                value = 0
                            elif pd.api.types.is_float_dtype(dtype):
                                value = 0.0
                            else:
                                value = ''  # ë¬¸ìì—´/ê¸°íƒ€ ìœ í˜•ì˜ ê¸°ë³¸ê°’

                        record_values.append(value)

                    w.record(*record_values)

            # ìƒì„±ëœ íŒŒì¼ ì••ì¶•
            zip_buffer = io.BytesIO()
            with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
                for file_path in Path(tmpdir).glob(f'{base_filename}.*'):
                    zip_file.write(file_path, arcname=file_path.name)
            zip_buffer.seek(0)
            return zip_buffer

        except Exception as e:
            st.error(f"pyshp ë¼ì´ë¸ŒëŸ¬ë¦¬ë¡œ SHP íŒŒì¼ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤: {e}")
            return None


# --- ë‹¤ìš´ë¡œë“œë¥¼ ìœ„í•œ ê¸°ë³¸ íŒŒì¼ ì´ë¦„ ì¤€ë¹„ ---
uploaded_file_name = st.session_state.get('uploaded_file_name', 'untitled')
base_filename = Path(uploaded_file_name).stem
timestamp = datetime.datetime.now().strftime('%Y%m%d%H%M%S')

plot_figures = {}
shp_buffers = {}

# --- 0. Matplotlib ê¸€ê¼´ êµ¬ì„± ---
if platform.system() == 'Windows':
    plt.rc('font', family='Malgun Gothic')
else:
    # Linuxì˜ ê²½ìš° 'NanumGothic'ì´ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤.
    # sudo apt-get install -y fonts-nanum*
    plt.rc('font', family='NanumGothic')
plt.rcParams['axes.unicode_minus'] = False

# --- 1. í˜ì´ì§€ ì„¤ì • ë° ìŠ¤íƒ€ì¼ë§ ---
st.set_page_config(page_title="ë¶„ì„ ê²°ê³¼ - ì§€í˜• ë¶„ì„ ì„œë¹„ìŠ¤",
                   page_icon="ğŸ“Š",
                   layout="wide",
                   initial_sidebar_state="collapsed")
apply_styles()

# --- 2. ì„¸ì…˜ ìƒíƒœ í™•ì¸ ---
if 'dem_results' not in st.session_state:
    st.warning("ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤. í™ˆ í˜ì´ì§€ë¡œ ëŒì•„ê°€ ë¶„ì„ì„ ë¨¼ì € ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    if st.button("í™ˆìœ¼ë¡œ ëŒì•„ê°€ê¸°"):
        st.switch_page("app.py")
    st.stop()

# --- 3. ì„¸ì…˜ì—ì„œ ë°ì´í„° ë¡œë“œ ---
dem_results = st.session_state.dem_results
selected_types = st.session_state.get('selected_analysis_types', [])
matched_sheets = st.session_state.get('matched_sheets', [])
analysis_date = datetime.datetime.now().strftime("%Y-%m-%d %H:%M")

# --- 4. í˜ì´ì§€ í—¤ë” ---
cols = st.columns([0.95, 0.05])
with cols[0]:
    st.markdown('''<div class="page-header" style="margin-top: -1.5rem;"><h1>ë¶„ì„ ê²°ê³¼</h1><p>ì„ íƒí•˜ì‹  í•­ëª©ì— ëŒ€í•œ ì§€í˜• ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤.</p></div>''',
                unsafe_allow_html=True)
with cols[1]:
    if st.button("ğŸ ", help="í™ˆ í™”ë©´ìœ¼ë¡œ ëŒì•„ê°‘ë‹ˆë‹¤.", use_container_width=True):
        # ì„¸ì…˜ ìƒíƒœë¥¼ ì§€ìš°ê¸° ì „ì— ì„ì‹œ TIF íŒŒì¼ ì •ë¦¬
        if 'dem_results' in st.session_state:
            for analysis_type in st.session_state.dem_results:
                results = st.session_state.dem_results.get(analysis_type, {})
                tif_path = results.get('tif_path')
                if tif_path and Path(tif_path).exists():
                    try:
                        Path(tif_path).unlink()
                    except OSError as e:
                        st.warning(f".tif íŒŒì¼ì„ ì‚­ì œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

        for key in list(st.session_state.keys()):
            if key != 'upload_counter':
                del st.session_state[key]
        st.switch_page("app.py")

# --- 6. 2D ë¶„ì„ ê²°ê³¼ (íƒ­) ---
st.markdown("### ğŸ“ˆ 2D ìƒì„¸ ë¶„ì„ ê²°ê³¼")
analysis_map = {
    'elevation': {'title': "í‘œê³  ë¶„ì„", 'unit': "m", 'binned_label': "í‘œê³  êµ¬ê°„ë³„ ë©´ì "},
    'slope': {'title': "ê²½ì‚¬ ë¶„ì„", 'unit': "Â°", 'binned_label': "ê²½ì‚¬ êµ¬ê°„ë³„ ë©´ì "},
    'aspect': {'title': "ê²½ì‚¬í–¥ ë¶„ì„", 'unit': "Â°", 'binned_label': "ê²½ì‚¬í–¥ êµ¬ê°„ë³„ ë©´ì "},
    'soil': {'title': "í† ì–‘ë„ ë¶„ì„", 'unit': "mÂ²", 'binned_label': "í† ì–‘ ì¢…ë¥˜ë³„ ë©´ì ", 'class_col': 'soilsy', 'legend_title': 'í† ì–‘ë„(Soilsy)'},
    'hsg': {'title': "ìˆ˜ë¬¸í•™ì  í† ì–‘êµ°", 'unit': "mÂ²", 'binned_label': "HSG ë“±ê¸‰ë³„ ë©´ì ", 'class_col': 'hg', 'legend_title': 'í† ì–‘êµ°(HSG)'},
    'landcover': {'title': "í† ì§€í”¼ë³µë„", 'unit': "mÂ²", 'binned_label': "í† ì§€í”¼ë³µë³„ ë©´ì ", 'class_col': 'l2_name', 'legend_title': 'í† ì§€í”¼ë³µë„(Landcover)'},
}
valid_selected_types = [t for t in selected_types if t in dem_results]

if not valid_selected_types:
    st.info("í‘œì‹œí•  2D ë¶„ì„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
else:
    # ê° ë¶„ì„ ìœ í˜•ì„ ë°˜ë³µí•˜ê³  ê²°ê³¼ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ í‘œì‹œí•©ë‹ˆë‹¤.
    for analysis_type in valid_selected_types:
        with st.container(border=True):
            st.markdown(
                f"### ğŸ“ˆ {analysis_map.get(analysis_type, {}).get('title', analysis_type)}")

            results = dem_results[analysis_type]
            stats = results.get('stats')
            grid = results.get('grid')
            gdf = results.get('gdf')

            # [ìµœì í™” 1] ë°ì´í„° ìœ í˜•ì„ ë³€ê²½í•˜ì—¬ ê·¸ë¦¬ë“œì˜ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ì ˆë°˜ìœ¼ë¡œ ì¤„ì…ë‹ˆë‹¤.
            if grid is not None:
                grid = grid.astype(np.float32)

                # [ìµœì í™” 2 & ì¶•ì²™ ë§‰ëŒ€ ìˆ˜ì •] ë™ì  ë‹¤ìš´ìƒ˜í”Œë§ ë° í”½ì…€ í¬ê¸° ì¡°ì •
                effective_pixel_size = st.session_state.get('pixel_size', 1.0)
                PIXEL_THRESHOLD = 5_000_000  # ì•½ 2500x2000 ì´ë¯¸ì§€
                if grid.size > PIXEL_THRESHOLD:
                    downsample_factor = (PIXEL_THRESHOLD / grid.size) ** 0.5
                    st.info(
                        f"â„¹ï¸ ë¶„ì„ ì˜ì—­ì´ ë§¤ìš° ì»¤ì„œ ì‹œê°í™” í•´ìƒë„ë¥¼ ì›ë³¸ì˜ {downsample_factor:.1%}ë¡œ ìë™ ì¡°ì •í•©ë‹ˆë‹¤.")
                    # order=1ì€ ì´ì¤‘ ì„ í˜• ë³´ê°„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.
                    grid = zoom(grid, downsample_factor, order=1)
                    # ì •í™•í•œ ì¶•ì²™ ë§‰ëŒ€ë¥¼ ìœ„í•´ ìƒˆ í•´ìƒë„ì™€ ì¼ì¹˜í•˜ë„ë¡ í”½ì…€ í¬ê¸° ì¡°ì •
                    effective_pixel_size = effective_pixel_size / downsample_factor

            if stats:
                st.markdown("#### ìš”ì•½ í†µê³„")
                cols = st.columns(3)
                cols[0].metric(
                    label="ìµœì†Œê°’", value=f"{stats.get('min', 0):.2f} {analysis_map.get(analysis_type, {}).get('unit', '')}")
                cols[1].metric(
                    label="ìµœëŒ€ê°’", value=f"{stats.get('max', 0):.2f} {analysis_map.get(analysis_type, {}).get('unit', '')}")
                cols[2].metric(
                    label="í‰ê· ê°’", value=f"{stats.get('mean', 0):.2f} {analysis_map.get(analysis_type, {}).get('unit', '')}")

                title = analysis_map.get(analysis_type, {}).get(
                    'title', analysis_type)
                with st.spinner(f"'{title}' ë¶„ì„ë„ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    # --- í†µí•© DEM ë¶„ì„ í”Œë¡œíŒ… ---

                    # ê²°ê³¼ ì‚¬ì „ì—ì„œ ëª¨ë“  ì‹œê°í™” ì •ë³´ ê²€ìƒ‰
                    bins = results.get('bins')
                    labels = results.get('labels')
                    palette_name = results.get('palette_name')

                    if not all([bins, labels, palette_name]):
                        st.warning(f"'{title}'ì— ëŒ€í•œ ì‹œê°í™” ì •ë³´ë¥¼ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue

                    palette_data = get_palette(palette_name)
                    if not palette_data:
                        st.warning(f"'{palette_name}' íŒ”ë ˆíŠ¸ë¥¼ DBì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                        continue

                    # ê²½ì‚¬í–¥ì˜ ê²½ìš°, ë°ì´í„° ì²˜ë¦¬ì™€ ì¼ì¹˜í•˜ë„ë¡ íŒ”ë ˆíŠ¸ë¥¼ ì¬ì •ë ¬í•˜ì—¬ 'Flat'ì„ ë¨¼ì € ë°°ì¹˜í•©ë‹ˆë‹¤.
                    if analysis_type == 'aspect':
                        flat_label_item = None
                        for item in palette_data:
                            if item['bin_label'].strip().lower() == 'flat':
                                flat_label_item = item
                                break

                        if flat_label_item:
                            palette_data.remove(flat_label_item)
                            palette_data.insert(0, flat_label_item)

                    colors = [item['hex_color'] for item in palette_data]

                    # ì»¬ëŸ¬ë§µ ë° ì •ê·œí™” ìƒì„±
                    cmap = ListedColormap(colors)
                    norm = BoundaryNorm(bins, cmap.N)

                    # --- í”Œë¡œíŒ… ---
                    fig, ax = create_padded_fig_ax(figsize=(10, 8))
                    ax.set_title(title, fontsize=16, pad=20)

                    use_hillshade = False
                    if analysis_type == 'elevation':
                        use_hillshade = st.toggle(
                            "ìŒì˜ê¸°ë³µë„ ì¤‘ì²©", value=True, key=f"hillshade_{analysis_type}")
                        if use_hillshade:
                            hillshade = create_hillshade(grid)
                            rgba_data = cmap(norm(grid))
                            intensity = 0.7
                            hillshade_adjusted = 0.5 + hillshade * intensity
                            for i in range(3):
                                rgba_data[:, :, i] *= hillshade_adjusted
                            valid_mask = ~np.isnan(grid)
                            rgba_data[~valid_mask, 3] = 0
                            ax.imshow(np.clip(rgba_data, 0, 1), origin='upper')
                            del hillshade, hillshade_adjusted, rgba_data
                            gc.collect()
                        else:
                            ax.imshow(np.ma.masked_invalid(
                                grid), cmap=cmap, norm=norm)
                    else:
                        ax.imshow(np.ma.masked_invalid(
                            grid), cmap=cmap, norm=norm)

                    # ë“±ê³ ì„  ì¶”ê°€ (50m ê°„ê²©)
                    if analysis_type == 'elevation':
                        min_val, max_val = stats.get(
                            'min', 0), stats.get('max', 1)
                        level_interval = 100
                        start_level = np.ceil(
                            min_val / level_interval) * level_interval
                        end_level = np.floor(
                            max_val / level_interval) * level_interval

                        if start_level < end_level:
                            contour_levels = np.arange(
                                start_level, end_level + 1, level_interval)
                        else:
                            contour_levels = np.linspace(min_val, max_val, 10)

                        contour = ax.contour(
                            grid, levels=contour_levels, colors='k', alpha=0.3, linewidths=0.7)

                        # ë“±ê³ ì„  ë¼ë²¨ ì¶”ê°€ (50m ê°„ê²©)
                        label_interval = 100
                        start_label_level = np.ceil(
                            min_val / label_interval) * label_interval
                        end_label_level = np.floor(
                            max_val / label_interval) * label_interval

                        if start_label_level < end_label_level:
                            label_levels = np.arange(
                                start_label_level, end_label_level + 1, label_interval)
                        else:
                            # 100m ê°„ê²© ë¼ë²¨ì„ í‘œì‹œí•  ìˆ˜ ì—†ìœ¼ë©´ ëª¨ë“  ë“±ê³ ì„ ì— ë¼ë²¨ í‘œì‹œ
                            label_levels = contour_levels

                        clabels = ax.clabel(
                            contour, levels=label_levels, inline=True, fontsize=8, fmt='%.0f')
                        # [ê°€ë…ì„± í–¥ìƒ] ë“±ê³ ì„  ë¼ë²¨ì— í°ìƒ‰ í…Œë‘ë¦¬ ì¶”ê°€
                        plt.setp(clabels, fontweight='bold', path_effects=[
                                 path_effects.withStroke(linewidth=3, foreground='w')])

                    # --- ë²”ë¡€ ë° ì§€ë„ ìš”ì†Œ ---
                    patches = [mpatches.Patch(color=color, label=label)
                               for color, label in zip(colors, labels)]
                    legend_titles = {
                        'elevation': 'í‘œê³ (Elevation)',
                        'slope': 'ê²½ì‚¬(Slope)',
                        'aspect': 'ê²½ì‚¬í–¥(Aspect)'
                    }
                    legend_title = legend_titles.get(analysis_type, 'ë²”  ë¡€')
                    legend = ax.legend(handles=patches, title=legend_title,
                                       bbox_to_anchor=(0.1, 0.1), loc='lower left',
                                       bbox_transform=fig.transFigure,
                                       fontsize='small', frameon=True, framealpha=1,
                                       edgecolor='black')
                    legend.get_title().set_fontweight('bold')

                    # ë²”ë¡€ íŒ¨ì¹˜ì— í…Œë‘ë¦¬ ìƒ‰ìƒ ë° ë„ˆë¹„ ê°•ì œ ì ìš©
                    for legend_patch in legend.get_patches():
                        legend_patch.set_edgecolor('black')
                        legend_patch.set_linewidth(0.7)

                    adjust_ax_limits(ax)
                    add_north_arrow(ax)
                    scale_params = calculate_accurate_scalebar_params(
                        effective_pixel_size, grid.shape, 50, fig, ax)
                    draw_accurate_scalebar(
                        fig, ax, effective_pixel_size, scale_params, grid.shape)
                    ax.axis('off')

                    # --- í‘œì‹œ ë° ë‹¤ìš´ë¡œë“œ ---
                    img_buffer = io.BytesIO()
                    fig.savefig(img_buffer, format='png',
                                bbox_inches='tight', dpi=150)
                    img_buffer.seek(0)
                    st.pyplot(fig)
                    plot_figures[analysis_type] = fig

            elif gdf is not None and not gdf.empty:
                title = analysis_map.get(
                    analysis_type, {}).get('title', analysis_type)
                with st.spinner(f"'{title}' ë¶„ì„ë„ë¥¼ ìƒì„±í•˜ëŠ” ì¤‘ì…ë‹ˆë‹¤..."):
                    fig, ax = create_padded_fig_ax(figsize=(10, 10))
                    type_info = analysis_map.get(analysis_type, {})
                    class_col = type_info.get('class_col')

                    # í† ì§€í”¼ë³µë„ ì‚¬ìš©ì ì •ì˜ ìƒ‰ìƒ ë¡œì§
                    if analysis_type == 'landcover':
                        # ì»¬ëŸ¬ë§µì´ ë¡œë“œë˜ì—ˆê³  í•„ìš”í•œ ì»¬ëŸ¼ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
                        color_map = get_landcover_colormap()
                        if color_map and 'l2_code' in gdf.columns and 'l2_name' in gdf.columns:
                            gdf['plot_color'] = gdf['l2_code'].map(color_map)
                            # ì§€ì •ëœ ìƒ‰ìƒìœ¼ë¡œ í”Œë¡¯, ì ì¬ì ì¸ ëˆ„ë½ ìƒ‰ìƒ ì²˜ë¦¬
                            gdf.plot(ax=ax, color=gdf['plot_color'].fillna(
                                '#FFFFFF'), linewidth=0.5, edgecolor='k')

                            # ì‚¬ìš©ì ì •ì˜ ë²”ë¡€ ìƒì„±
                            unique_cats = gdf[['l2_code', 'l2_name']].drop_duplicates(
                            ).sort_values(by='l2_code')
                            patches = []
                            for _, row in unique_cats.iterrows():
                                code = row['l2_code']
                                name = row['l2_name']
                                # ë§µì— ì—†ìœ¼ë©´ í°ìƒ‰ìœ¼ë¡œ ê¸°ë³¸ê°’ ì„¤ì •
                                color = color_map.get(code, '#FFFFFF')
                                patch = mpatches.Patch(
                                    color=color, label=f'{name}')
                                patches.append(patch)

                            if patches:
                                n_items = len(patches)
                                n_cols = (n_items + 19) // 20
                                legend = ax.legend(handles=patches, title=type_info.get('legend_title', 'ë¶„ë¥˜'),
                                                   bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small',
                                                   ncol=n_cols)
                                # ë²”ë¡€ íŒ¨ì¹˜ì— í…Œë‘ë¦¬ ìƒ‰ìƒ ë° ë„ˆë¹„ ê°•ì œ ì ìš©
                                for legend_patch in legend.get_patches():
                                    legend_patch.set_edgecolor('black')
                                    legend_patch.set_linewidth(0.7)
                        else:
                            # ì‚¬ìš©ì ì •ì˜ ìƒ‰ìƒ ì‹¤íŒ¨ ì‹œ í† ì§€í”¼ë³µë„ ëŒ€ì²´ ë¡œì§
                            gdf.plot(column=class_col, ax=ax, legend=True, categorical=True,
                                     legend_kwds={'title': type_info.get('legend_title', 'ë¶„ë¥˜'), 'bbox_to_anchor': (1.05, 1), 'loc': 'upper left'})

                    # ë‹¤ë¥¸ ë²¡í„° ìœ í˜•(í† ì–‘, hsg)ì— ëŒ€í•œ ë¡œì§
                    else:
                        if class_col and class_col in gdf.columns:
                            gdf_plot = gdf[gdf[class_col].notna()].copy()
                            unique_cats = sorted(gdf_plot[class_col].unique())

                            num_cats = len(unique_cats)
                            # ìµœëŒ€ 20ê°œ ë²”ì£¼ì—ëŠ” 'tab20'ì„ ì‚¬ìš©í•˜ê³ , ê·¸ ì´ìƒì€ ìƒ˜í”Œë§ëœ ì»¬ëŸ¬ë§µì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
                            if num_cats <= 20:
                                cmap = plt.cm.get_cmap('tab20', num_cats)
                                colors = cmap.colors
                            else:
                                cmap = plt.cm.get_cmap('viridis', num_cats)
                                colors = cmap(np.linspace(0, 1, num_cats))

                            color_map = {cat: color for cat,
                                         color in zip(unique_cats, colors)}
                            gdf_plot['plot_color'] = gdf_plot[class_col].map(
                                color_map)

                            gdf_plot.plot(
                                ax=ax, color=gdf_plot['plot_color'], linewidth=0.5, edgecolor='k')

                            patches = [mpatches.Patch(
                                color=color, label=cat) for cat, color in color_map.items()]

                            if patches:
                                n_items = len(patches)
                                n_cols = (n_items + 19) // 20
                                legend = ax.legend(handles=patches, title=type_info.get('legend_title', 'ë¶„ë¥˜'),
                                                   bbox_to_anchor=(1.05, 1), loc='upper left', fontsize='small',
                                                   ncol=n_cols)
                                # ë²”ë¡€ íŒ¨ì¹˜ì— í…Œë‘ë¦¬ ìƒ‰ìƒ ë° ë„ˆë¹„ ê°•ì œ ì ìš©
                                for legend_patch in legend.get_patches():
                                    legend_patch.set_edgecolor('black')
                                    legend_patch.set_linewidth(0.7)
                        else:
                            gdf.plot(ax=ax)
                            if class_col:
                                st.warning(
                                    f"ì£¼ì˜: ë¶„ì„ì— í•„ìš”í•œ ë¶„ë¥˜ ì»¬ëŸ¼ '{class_col}'ì„(ë¥¼) ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DB í…Œì´ë¸” ìŠ¤í‚¤ë§ˆë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”. ì‚¬ìš© ê°€ëŠ¥í•œ ì»¬ëŸ¼: {list(gdf.columns)}")
                            else:
                                st.warning("ë¶„ì„ ìœ í˜•ì— ëŒ€í•œ ë¶„ë¥˜ ì»¬ëŸ¼ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")

                    adjust_ax_limits(ax)
                    add_north_arrow(ax)

                    # --- DEM ìŠ¤íƒ€ì¼ê³¼ ì¼ì¹˜í•˜ë„ë¡ ì¶•ì²™ ë§‰ëŒ€ í†µí•© ---
                    # 1. í”Œë¡¯ ì¶•ì—ì„œ ë¯¸í„° ë‹¨ìœ„ì˜ ì§€ë„ ì¹˜ìˆ˜ ê°€ì ¸ì˜¤ê¸°
                    x_min, x_max = ax.get_xlim()
                    map_width_m = x_max - x_min

                    # 2. ì´ë¯¸ì§€ ëª¨ì–‘ ë° í”½ì…€ í¬ê¸°ì— ëŒ€í•œ í”„ë¡ì‹œ ìƒì„±
                    #    (savefig dpi ë° figsize ê¸°ë°˜)
                    dpi = 150
                    # create_padded_fig_axì—ì„œ ì‚¬ìš©ëœ figsize
                    figsize_w, figsize_h = (10, 10)
                    proxy_img_width_px = int(figsize_w * dpi)
                    proxy_img_height_px = int(figsize_h * dpi)
                    proxy_img_shape = (proxy_img_height_px, proxy_img_width_px)

                    # effective_pixel_sizeëŠ” ë¯¸í„°/í”½ì…€ì…ë‹ˆë‹¤.
                    if proxy_img_width_px > 0:
                        effective_pixel_size = map_width_m / proxy_img_width_px
                    else:
                        effective_pixel_size = 1.0  # ëŒ€ì²´

                    # 3. ì •í™•í•œ ì¶•ì²™ ë§‰ëŒ€ ê³„ì‚° ë° ê·¸ë¦¬ê¸°
                    scale_params = calculate_accurate_scalebar_params(
                        effective_pixel_size, proxy_img_shape, 50, fig, ax)
                    draw_accurate_scalebar(
                        fig, ax, effective_pixel_size, scale_params, proxy_img_shape)
                    ax.axis('off')

                    # --- í‘œì‹œ ë° ë²„í¼ ì €ì¥ ---
                    img_buffer = io.BytesIO()
                    fig.savefig(img_buffer, format='png',
                                bbox_inches='tight', dpi=150)
                    img_buffer.seek(0)

                    st.pyplot(fig)
                    plot_figures[analysis_type] = fig

                    # --- SHP íŒŒì¼ ë²„í¼ ìƒì„± ë° ì €ì¥ ---
                    shp_zip_buffer = create_shapefile_zip(
                        gdf, f"{analysis_type}_{base_filename}"
                    )
                    if shp_zip_buffer:
                        shp_buffers[analysis_type] = {
                            "buffer": shp_zip_buffer,
                            "title": type_info.get('title', analysis_type)
                        }
                    # --- ë¡œì§ ì™„ë£Œ ---

            else:
                st.info("ì‹œê°í™”í•  2D ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
            st.markdown("---")  # ë¶„ì„ ì‚¬ì´ì— êµ¬ë¶„ì„  ì¶”ê°€
# --- 7. ìš”ì•½ ë° ìµœì¢… ë‹¤ìš´ë¡œë“œ ---
st.markdown("### ğŸ“‹ ìƒì„¸ ë¶„ì„ ë³´ê³ ì„œ")

# --- TXT ìš”ì•½(í‘œì‹œìš©) ë° CSV(ë‹¤ìš´ë¡œë“œìš©) ë°ì´í„° ìƒì„± ---
summary_lines = []
csv_data = []

pixel_size = st.session_state.get('pixel_size', 1.0)
area_per_pixel = pixel_size * pixel_size

# --- ë³´ê³ ì„œ í—¤ë”ì˜ ì´ ë©´ì  ê³„ì‚° ---
# [ì¤‘ìš” ìˆ˜ì •] ì›ë³¸ ì‚¬ìš©ì ì œê³µ ì§€ì˜¤ë©”íŠ¸ë¦¬ì˜ ë©´ì ì„ ìœ ì¼í•œ ê¸°ì¤€ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.
# ì´ë ‡ê²Œ í•˜ë©´ ë³´ê³ ì„œì˜ ëª¨ë“  ë¶€ë¶„ì—ì„œ ì´ ë©´ì ì´ ì¼ê´€ë˜ê²Œ ìœ ì§€ë©ë‹ˆë‹¤.
report_total_area_m2 = 0
if 'gdf' in st.session_state and not st.session_state.gdf.empty:
    # í•©ì‚°í•˜ê¸° ì „ì— 'area' ì»¬ëŸ¼ì´ ìˆëŠ”ì§€ í™•ì¸
    if 'area' not in st.session_state.gdf.columns:
        st.session_state.gdf['area'] = st.session_state.gdf.geometry.area
    report_total_area_m2 = st.session_state.gdf.area.sum()
else:
    # ì£¼ GDFê°€ ì—†ëŠ” ë“œë¬¸ ê²½ìš°ì— ëŒ€í•œ ëŒ€ì²´ ë¡œì§
    area_source_type = None
    if 'elevation' in valid_selected_types:
        area_source_type = 'elevation'
    elif valid_selected_types:
        area_source_type = valid_selected_types[0]

    if area_source_type:
        results = dem_results[area_source_type]
        stats = results.get('stats')
        gdf = results.get('gdf')
        if stats:
            report_total_area_m2 = stats.get('area', 0) * area_per_pixel
        elif gdf is not None and not gdf.empty:
            if 'area' not in gdf.columns:
                gdf['area'] = gdf.geometry.area
            report_total_area_m2 = gdf.area.sum()

# ì¼ë°˜ ì •ë³´
summary_lines.append(f"ë¶„ì„ ì¼ì‹œ: {analysis_date}")
summary_lines.append(
    f"ë¶„ì„ ëŒ€ìƒ: {st.session_state.get('uploaded_file_name', 'N/A')}")
csv_data.append({'ë¶„ì„ êµ¬ë¶„': 'ê¸°ë³¸ ì •ë³´', 'í•­ëª©': 'ë¶„ì„ ì¼ì‹œ',
                'ê°’': analysis_date, 'ë‹¨ìœ„': '', 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})
csv_data.append({'ë¶„ì„ êµ¬ë¶„': 'ê¸°ë³¸ ì •ë³´', 'í•­ëª©': 'ë¶„ì„ ëŒ€ìƒ', 'ê°’': st.session_state.get(
    'uploaded_file_name', 'N/A'), 'ë‹¨ìœ„': '', 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})

if len(matched_sheets) > 20:
    summary_lines.append(f"ì‚¬ìš©ëœ ë„ì—½: {len(matched_sheets)}ê°œ")
    csv_data.append({'ë¶„ì„ êµ¬ë¶„': 'ê¸°ë³¸ ì •ë³´', 'í•­ëª©': 'ì‚¬ìš©ëœ ë„ì—½ ê°œìˆ˜', 'ê°’': len(
        matched_sheets), 'ë‹¨ìœ„': 'ê°œ', 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})
else:
    summary_lines.append(
        f"ì‚¬ìš©ëœ ë„ì—½: {len(matched_sheets)}ê°œ ({', '.join(matched_sheets)})")
    csv_data.append({'ë¶„ì„ êµ¬ë¶„': 'ê¸°ë³¸ ì •ë³´', 'í•­ëª©': 'ì‚¬ìš©ëœ ë„ì—½',
                    'ê°’': f"{len(matched_sheets)}ê°œ ({', '.join(matched_sheets)})", 'ë‹¨ìœ„': '', 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})

summary_lines.append(f"ì´ ë¶„ì„ ë©´ì : {int(report_total_area_m2):,} mÂ²")
csv_data.append({'ë¶„ì„ êµ¬ë¶„': 'ê¸°ë³¸ ì •ë³´', 'í•­ëª©': 'ì´ ë¶„ì„ ë©´ì ', 'ê°’': '', 'ë‹¨ìœ„': 'mÂ²',
                'ë©´ì (mÂ²)': f"{int(report_total_area_m2):,}", 'ë¹„ìœ¨(%)': ''})
summary_lines.append("")

# ë¶„ì„ë³„ ì •ë³´
for analysis_type in valid_selected_types:
    stats = dem_results[analysis_type].get('stats')
    binned_stats = dem_results[analysis_type].get('binned_stats')
    gdf = dem_results[analysis_type].get('gdf')
    title_info = analysis_map.get(analysis_type, {})
    title = title_info.get('title', analysis_type)

    summary_lines.append(f"--- {title} ---")

    total_area_m2 = 0
    if stats:
        total_area_m2 = stats.get('area', 0) * area_per_pixel
        unit = title_info.get('unit', '')
        summary_lines.append(f"- ìµœì†Œê°’: {stats.get('min', 0):.2f} {unit}")
        summary_lines.append(f"- ìµœëŒ€ê°’: {stats.get('max', 0):.2f} {unit}")
        summary_lines.append(f"- í‰ê· ê°’: {stats.get('mean', 0):.2f} {unit}")

        csv_data.append({'ë¶„ì„ êµ¬ë¶„': title, 'í•­ëª©': 'ìµœì†Œê°’',
                        'ê°’': f"{stats.get('min', 0):.2f}", 'ë‹¨ìœ„': unit, 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})
        csv_data.append({'ë¶„ì„ êµ¬ë¶„': title, 'í•­ëª©': 'ìµœëŒ€ê°’',
                        'ê°’': f"{stats.get('max', 0):.2f}", 'ë‹¨ìœ„': unit, 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})
        csv_data.append({'ë¶„ì„ êµ¬ë¶„': title, 'í•­ëª©': 'í‰ê· ê°’',
                        'ê°’': f"{stats.get('mean', 0):.2f}", 'ë‹¨ìœ„': unit, 'ë©´ì (mÂ²)': '', 'ë¹„ìœ¨(%)': ''})

    elif gdf is not None and not gdf.empty:
        if 'area' not in gdf.columns:
            gdf['area'] = gdf.geometry.area
        total_area_m2 = gdf.area.sum()

    if binned_stats:
        summary_lines.append(f"\n[{title_info.get('binned_label', 'êµ¬ê°„ë³„ í†µê³„')}]")
        for row in binned_stats:
            binned_area_m2 = row['area'] * area_per_pixel
            percentage = (binned_area_m2 / total_area_m2 *
                          100) if total_area_m2 > 0 else 0
            summary_lines.append(
                f"- {row['bin_range']}: {int(binned_area_m2):,} mÂ² ({percentage:.1f} %)")
            csv_data.append({'ë¶„ì„ êµ¬ë¶„': title, 'í•­ëª©': row['bin_range'], 'ê°’': '', 'ë‹¨ìœ„': '',
                            'ë©´ì (mÂ²)': f"{int(binned_area_m2):,}", 'ë¹„ìœ¨(%)': f"{percentage:.1f}"})

    if gdf is not None and not gdf.empty:
        class_col = title_info.get('class_col')
        if class_col and class_col in gdf.columns:
            # [ì¤‘ìš” ìˆ˜ì • 2] ë¨¼ì € dissolveë¥¼ ì‚¬ìš©í•˜ì—¬ ì§€ì˜¤ë©”íŠ¸ë¦¬ë¥¼ ë³‘í•©í•œ ë‹¤ìŒ ë©´ì ì„ ê³„ì‚°í•©ë‹ˆë‹¤.
            # ì´ë ‡ê²Œ í•˜ë©´ ì†ŒìŠ¤ ë°ì´í„°ì˜ ì¤‘ì²©ëœ í´ë¦¬ê³¤ì„ ì˜¬ë°”ë¥´ê²Œ ì²˜ë¦¬í•©ë‹ˆë‹¤.
            dissolved_gdf = gdf.dissolve(by=class_col)
            dissolved_gdf['area'] = dissolved_gdf.geometry.area
            dissolved_gdf = dissolved_gdf.sort_values(by='area', ascending=False)

            summary_lines.append(
                f"\n[{title_info.get('binned_label', 'ì¢…ë¥˜ë³„ í†µê³„')}]")
            for item, row in dissolved_gdf.iterrows():
                area = row['area']
                percentage = (area / total_area_m2 *
                              100) if total_area_m2 > 0 else 0
                summary_lines.append(
                    f"- {item}: {int(area):,} mÂ² ({percentage:.1f} %)")
                csv_data.append({'ë¶„ì„ êµ¬ë¶„': title, 'í•­ëª©': item, 'ê°’': '', 'ë‹¨ìœ„': '',
                                'ë©´ì (mÂ²)': f"{int(area):,}", 'ë¹„ìœ¨(%)': f"{percentage:.1f}"})
        else:
            # ì´ ë¶€ë¶„ì€ GDFì˜ ì´ ë©´ì ì´ ì´ë¯¸ ìœ„ì—ì„œ ê³„ì‚°ë˜ì—ˆê¸° ë•Œë¬¸ì— ê¹Œë‹¤ë¡­ìŠµë‹ˆë‹¤.
            # ì£¼ ë³´ê³ ì„œ í—¤ë”ì™€ì˜ í˜¼ë™ì„ í”¼í•˜ê¸° ìœ„í•´ ë‹¤ë¥¸ 'ì´ ë©´ì ' ì¤„ì„ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.
            if class_col:
                summary_lines.append(
                    f"- (ìƒì„¸ ë©´ì  í†µê³„ë¥¼ ê³„ì‚°í•˜ë ¤ë©´ '{class_col}' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.)")

    summary_lines.append("")

# --- í‘œì‹œ í…ìŠ¤íŠ¸ ë° CSV ë¬¸ìì—´ ìƒì„± ---
summary_text = "\n".join(summary_lines)
report_df = pd.DataFrame(csv_data)
csv_buffer = io.StringIO()
report_df.to_csv(csv_buffer, index=False, encoding='utf-8-sig')
csv_string = csv_buffer.getvalue()


# ìš”ì•½ í…ìŠ¤íŠ¸ ì˜ì—­ í‘œì‹œ
st.text_area("", summary_text, height=400)

# --- ìµœì¢… ZIP ìƒì„± ë° ë‹¤ìš´ë¡œë“œ ë²„íŠ¼ ---
zip_buffer = io.BytesIO()
with zipfile.ZipFile(zip_buffer, 'w', zipfile.ZIP_DEFLATED) as zip_file:
    # zipì— ìš”ì•½ CSV ì¶”ê°€
    zip_file.writestr(
        f"analysis_summary_{timestamp}.csv", csv_string)

    # zipì— í”Œë¡¯ ì´ë¯¸ì§€ ì¶”ê°€
    for analysis_type, fig in plot_figures.items():
        img_buffer = io.BytesIO()

        # ì˜¬ë°”ë¥¸ íŒŒì¼ ì´ë¦„ì„ ìœ„í•´ í† ê¸€ ìƒíƒœ ë‹¤ì‹œ í™•ì¸
        use_hillshade = st.session_state.get(
            f"hillshade_{analysis_type}", False)
        file_name_suffix = "_hillshade" if analysis_type == 'elevation' and use_hillshade else ""

        fig.savefig(img_buffer, format='png', bbox_inches='tight', dpi=150)
        img_buffer.seek(0)

        zip_file.writestr(
            f"{analysis_type}_analysis{file_name_suffix}.png", img_buffer.getvalue())
        plt.close(fig)  # ì €ì¥ í›„ ê·¸ë¦¼ ë‹«ê¸°

    # zipì— ë¶„ì„ TIF íŒŒì¼ ì¶”ê°€
    for analysis_type in valid_selected_types:
        results = dem_results.get(analysis_type, {})
        tif_path = results.get('tif_path')
        if tif_path and Path(tif_path).exists():
            zip_file.write(tif_path, arcname=f"{analysis_type}.tif")

    # zipì˜ ì••ì¶•ì„ í’€ê³  ë‚´ìš©ì„ ë‹¤ì‹œ ì¶”ê°€í•˜ì—¬ zipì— SHP íŒŒì¼ ì¶”ê°€
    for analysis_type, data in shp_buffers.items():
        inner_zip_buffer = data["buffer"]
        with zipfile.ZipFile(inner_zip_buffer, 'r') as inner_zip:
            for file_info in inner_zip.infolist():
                # íŒŒì¼ì„ í•˜ìœ„ ë””ë ‰í† ë¦¬ì— ë„£ì§€ ì•Šìœ¼ë ¤ë©´ ì§ì ‘ ì‘ì„±í•©ë‹ˆë‹¤.
                zip_file.writestr(file_info.filename, inner_zip.read(file_info.filename))

zip_buffer.seek(0)

# --- 8. ìµœì¢… ë‹¤ìš´ë¡œë“œ ì„¹ì…˜ ---
st.markdown("### ğŸ“¥ ë‹¤ìš´ë¡œë“œ")

# --- ëª¨ë“  ë‹¤ìš´ë¡œë“œ í•­ëª© ëª©ë¡ ìƒì„± ---
download_items = []

# ì£¼ zip ë²„íŠ¼ì— ëŒ€í•œ ë™ì  ë¼ë²¨ ë° ë„ì›€ë§ í…ìŠ¤íŠ¸ ì •ì˜
main_zip_label = "ğŸ“¥ ì‹œê°í™”ìë£Œ+ë¶„ì„ë³´ê³ ì„œ (ZIP)"
main_zip_help = "ë¶„ì„ ë¦¬í¬íŠ¸, ëª¨ë“  ë¶„ì„ë„(PNG), ëª¨ë“  ì›ë³¸ ë¶„ì„ íŒŒì¼(TIF)ì„ í•œë²ˆì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."

if shp_buffers:  # SHP íŒŒì¼ì´ ìƒì„±ë˜ì–´ í¬í•¨ëœ ê²½ìš°
    main_zip_label = "ğŸ“¥ ì‹œê°í™”ìë£Œ+ë¶„ì„ë³´ê³ ì„œ+SHP (ZIP)"
    main_zip_help = "ë¶„ì„ ë¦¬í¬íŠ¸, ëª¨ë“  ë¶„ì„ë„(PNG), ëª¨ë“  ì›ë³¸ ë¶„ì„ íŒŒì¼(TIF), ë²¡í„° ë°ì´í„°(SHP)ë¥¼ í•œë²ˆì— ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."

# ì£¼ ZIP ë‹¤ìš´ë¡œë“œë¥¼ ë¨¼ì € ì¶”ê°€
download_items.append({
    "label": main_zip_label,
    "data": zip_buffer,
    "file_name": f"analysis_results_{base_filename}_{timestamp}.zip",
    "mime": "application/zip",
    "key": "main_zip_download",
    "help": main_zip_help
})

# ê°œë³„ SHP ë‹¤ìš´ë¡œë“œëŠ” ì´ì œ ì£¼ zipì— í¬í•¨ë˜ë¯€ë¡œ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.
# for analysis_type, data in shp_buffers.items():
#     download_items.append({
#         "label": f"ğŸ“¥ {data['title']} (SHP)",
#         "data": data['buffer'],
#         "file_name": f"{analysis_type}_{base_filename}_{timestamp}.zip",
#         "mime": "application/zip",
#         "key": f"shp_download_bottom_{analysis_type}",
#         "help": f"{data['title']} ë¶„ì„ ê²°ê³¼ë¥¼ SHP íŒŒì¼ë¡œ ë‹¤ìš´ë¡œë“œí•©ë‹ˆë‹¤."
#     })

# --- ì—´ ìƒì„± ë° ë²„íŠ¼ í‘œì‹œ ---
if download_items:
    cols = st.columns(len(download_items))
    for i, item in enumerate(download_items):
        with cols[i]:
            st.download_button(
                label=item["label"],
                data=item["data"],
                file_name=item["file_name"],
                mime=item["mime"],
                key=item["key"],
                use_container_width=True,
                help=item["help"]
            )

st.markdown("")  # ìŠ¤í˜ì´ì„œ

# --- ìµœì¢… ë²„íŠ¼ ---
if st.button("ìƒˆë¡œìš´ ë¶„ì„ ì‹œì‘í•˜ê¸°", use_container_width=True):
    # ì„¸ì…˜ ìƒíƒœë¥¼ ì§€ìš°ê¸° ì „ì— ì„ì‹œ TIF íŒŒì¼ ì •ë¦¬
    if 'dem_results' in st.session_state:
        for analysis_type in st.session_state.dem_results:
            results = st.session_state.dem_results.get(analysis_type, {})
            tif_path = results.get('tif_path')
            if tif_path and Path(tif_path).exists():
                try:
                    Path(tif_path).unlink()
                except OSError as e:
                    st.warning(f".tif íŒŒì¼ì„ ì‚­ì œí•˜ëŠ” ë° ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤: {e}")

    for key in list(st.session_state.keys()):
        if key not in ['upload_counter']:
            del st.session_state[key]
    st.switch_page("app.py")